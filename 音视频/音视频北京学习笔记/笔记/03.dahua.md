# 前言瞎扯

实际关于利用`FFmpeg`+`AudioUnit`,相关文章是有的也还不错但是没有给出完整[Demo]() , 所以我还是要写这么一篇, 闲扯中让各位(让我自己~)从`最基本的概念`->`能搞出东西`

本文有些关键词, 基础知识不太熟的同学看看我的这篇文章

[=====>音视频基础知识](http://www.codertomwu.com/2018/08/06/videoAduio00/)

只是为看懂本文的话, 看音频就好啦. 



# 看我这篇文章你能干嘛?

你可以完成一个音频播放Demo. 用AudioUnit播放一个`mp3`, `aac`, 这样的文件, 或者视频文件的音频也就是说只播放MP4文件声音.  播放一帧一帧的音频数据(实际上是音频裸数据PCM, 而PCM是没有`帧`的概念的.PCM说的采样..). 播放本地文件呢,是为后面播放网络过来的数据打个基础, 因为`解码`,`解封装`, `AudioUnit 相关API`等相关知识是直播也好播本地文件也好是相同的代码, 多的只是处理网络流部分的逻辑.


# 大概怎么做?
FFmpeg解码`mp3`, `aac`, `MP4`, 这类的`封装格式`拿到`裸数据(pcm)`, 然后`喂`给`AudioUnit`


# 材料

`FFmpeg` + `AudioUnit` + `音视频文件`

`FFmpeg`是编译iOS能用的静态库文件如图

![fffmpeg_iOS.png](https://upload-images.jianshu.io/upload_images/571446-14dee043fe79bfc0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

看看我这个文章,如果你本地没有编译好的.
[===>编译iOS能用的FFmpeg静态库](http://www.codertomwu.com/2018/08/05/ffmpegForiOS/)



往下就是具体逻辑讲解了, 默认你懂了关于音频的基础知识和已经编译好iOS能用的静态库了哈, 那啥要不再看看
[===>编译iOS能用的FFmpeg静态库](http://www.codertomwu.com/2018/08/05/ffmpegForiOS/)

[=====>音视频基础知识](http://www.codertomwu.com/2018/08/06/videoAduio00/)

# 1.AudioUnit


## 1.1大概原理闲扯

啥也不说. 看看一幅图.

![AudioUnitJG.png](https://upload-images.jianshu.io/upload_images/571446-5b8f8debc7ef80ef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

嗯嗯看看图,AudioUnit在下去就是硬件了.用它处理音视频数据确实略微"复杂"."复杂"的话功能就会有点骚.


 `AudioUnit 就一个小孩, 需要一直喂东西.我要做的就是不断喂他东西.`....或者说`AudioUnit就是一台机器,它生产的产品是声音, 我们要做的就是不断的给他填原料`, 本篇文章就当他是`打米机`好了, `FFmpeg`就是水稻收割机.

![FFmpeg_AudioUnit.png](https://upload-images.jianshu.io/upload_images/571446-9c327ba8ceabef27.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

如上图`水稻收割机`(`FFmpeg`)从`田里`(`音视频文件`)收获`稻谷`(`PCM`),然后进过我们调度给`打米机`(`AudioUnit`),然后生产`大米`(`声音`)..城里的同学请自行研究`水稻`-->`稻谷`-->`大米`全过程.

`打米机`如图右边那个漏斗是填稻谷的, 然后下面中间的出口产生大米,右边产生米糠(稻谷的壳). 当我们买来零件组装好一台打米机插上电就可以让它运行起来你要是填稻米它就生产大米,你没稻米填给它就在那白跑着浪费电,`打米机`它不管`稻米`哪来的它只要人给它填稻米,是不是`水稻收割机`从田里采集的还是农民通过人工采集的它不管, 它只是说`给我稻米给我电我给你大米`. 然后`AudioUnit` 这家伙跟它一个意思.



如图,AudioUnit跟打米机一样也是一个漏斗填音频(aac,pcm)数据给他,然后它让扬声器或者耳机出声.
![audioIO.png](https://upload-images.jianshu.io/upload_images/571446-68b6eb3b555843c3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 1.2 相关API混脸熟

好啦废话说了那么多了,基本上知道AudioUnit是一个什么样尿性的家伙了.下面说具体的类、结构体、函数、方法什么的了.

原料有:`AVAudioSession`, `AudioComponentDescription`,  `AUNode`, `AUGraph`, `AudioStreamBasicDescription`, `AURenderCallbackStruct` 差不多这些结构体类啥的(并不是~),

函数方法~(先写两个):
```
AUGraphNodeInfo(	AUGraph									inGraph,
					AUNode									inNode,
					AudioComponentDescription * __nullable	outDescription,
					AudioUnit __nullable * __nullable		outAudioUnit)		__OSX_AVAILABLE_STARTING(__MAC_10_5,__IPHONE_2_0);




AudioUnitSetProperty(				AudioUnit				inUnit,
									AudioUnitPropertyID		inID,
									AudioUnitScope			inScope,
									AudioUnitElement		inElement,
									const void * __nullable	inData,
									UInt32					inDataSize)				
												__OSX_AVAILABLE_STARTING(__MAC_10_0,__IPHONE_2_0);
```

开始有点代码了哈, 上面提到的类结构体方法函数先混个脸熟吧, 花30秒过一遍....
 

## 1.3操作过程和具体API讲解

AudioUnit的使用一句话讲解是这样的: 首先使用`AVAudioSession `会话用来管理获取硬件信息, 然后利用一个描述结构体(`AudioComponentDescription`)确定`AudioUnit`的类型(AudioUnit能做很多事情的,不同的类型干不同的事,我们这里是找能播放音频的那个),然后通过 `AUNode`, `AUGraph`拿到我们的`AudioUnit`, 最后设置`AudioUnit`的入口出口.

![AudioUnitSet.png](https://upload-images.jianshu.io/upload_images/571446-7da2a652c819414c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


### 1.3.1 AVAudioSession

在iOS的音视频开发中, 使用具体API之前都会先创建一个会话, 这里也不例外.这是必须的第一步, 你在使用AudioUnit之前必须先创建会话并设置相关参数.

AVAudioSession 用于管理与获取iOS设备音频的硬件信息, 并且是以单例的形式存在.iOS7以前是使用`Audio Session `两个实际上是干一件事.就是`管理与获取iOS设备音频的硬件信息`, 你的声音是扬声器播勒还是耳机了,是蓝牙耳机了还是插线耳机了这些信息都由他管, 举个例子:你用扬声器播的好好的然后你插耳机了这时要他做一定逻辑处理. 

AVAudioSession
```

AVAudioSession * audioSession = [AVAudioSession  sharedInstance];

```

Audio Session
```

AudioSessionInitialize(
                               NULL,// Run loop (NULL = main run loop)
                               kCFRunLoopDefaultMode, // Run loop mode
                               (void(*)(void*,UInt32))XXXXXX, // Interruption callback
                               NULL);    

```

`AVAudioSession` 和 `Audio Session`一个是类,一个是一个API函数,使用起来还是很不同的, 我们这里用前者. 我们将用一个包装类来使用AVAudioSession, 下面是具体介绍

- 1.获取AVAudioSession实例

```

AVAudioSession * audioSession = [AVAudioSession  sharedInstance];

```

- 2.设置硬件能力

我们要做什么? 看我的标题,我们只要播放声音,我们想要iPhone手机播放声音.然后我们设置`AVAudioSessionCategoryPlayback`, 如果我们要手机采集又播放就是`AVAudioSessionCategoryPlayAndRecord`


```

[audioSession setCategory:AVAudioSessionCategoryPlayback];

```


- 3. 设置I/O的Buffer, `Buffer `越小则说明延迟越低

![damijiBuffer.png](https://upload-images.jianshu.io/upload_images/571446-5bffc3f9e55a6ddb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

`AudioUnit`的buffer就好像打米机的稻谷漏斗. 如图打米机自带的漏斗填满稻谷可能需要1分钟打完, 所以我们需要快1分钟后就要再往里面填稻谷, 如果我们换成左边那个更小的漏斗(`buffer`)可能40秒就打完了, 换个大的就时间长点. 小的漏斗呢就需要人不断的加稻谷, 大的就不需要那么频繁.


```

 [audioSession setPreferredIOBufferDuration:bufferDuration error:nil];


```

PCM数据是1024个采样一个包, 所以最大值是1024采样点的时间, 所以这里的值最大是1024/sampleRate(采样率), 只能比这个小, 越小的buffer, 延迟就越低, 一般设置成`1024/sampleRate(采样率)`就行了.
如果采样率是44100, 就是1024/44100=0.023, 具体看采样率.
采样率哪来?FFmpeg读音视频文件得到.FFmpeg给的.

具体体现函数

```

/**

这就是我们给AudioUnit喂食的函数, 也就是AudioUnit的漏斗,你上buffer设置的越小呢AudioUnit调用这个函数的频率就越高, 然后每次问你要的inNumberFrames个数就越少, 多少的基础标准就是"1024/sampleRate"的值,实际上最大可以是"1024/sampleRate*1.4", 最小嘛就是"1.0/sampleRate"就是1buffer大小

AURenderCallbackStruct callbackStruct;
callbackStruct.inputProc = &STInputRenderCallback;
*/

typedef OSStatus
(*AURenderCallback)(	void *							inRefCon,
						AudioUnitRenderActionFlags *	ioActionFlags,
						const AudioTimeStamp *			inTimeStamp,
						UInt32							inBusNumber,
						UInt32							inNumberFrames,
						AudioBufferList * __nullable	ioData);

```


















